{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by including the functions to generate frequent itemsets (via the Apriori algorithm) and resulting association rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2016 Everaldo Aguiar & Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)\n",
    "#\n",
    "# Functions to compute and extract association rules from a given frequent itemset \n",
    "# generated by the Apriori algorithm.\n",
    "#\n",
    "# The Apriori algorithm is defined by Agrawal and Srikant in:\n",
    "# Fast algorithms for mining association rules\n",
    "# Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(filename):\n",
    "    '''Loads an example of market basket transactions from a provided csv file.\n",
    "\n",
    "    Returns: A list (database) of lists (transactions). Each element of a transaction is \n",
    "    an item.\n",
    "    '''\n",
    "\n",
    "    with open(filename,'r') as dest_f:\n",
    "        data_iter = csv.reader(dest_f, delimiter = ',', quotechar = '\"')\n",
    "        data = [data for data in data_iter]\n",
    "        data_array = np.asarray(data)\n",
    "        \n",
    "    return data_array\n",
    "\n",
    "def apriori(dataset, min_support=0.5, verbose=False):\n",
    "    \"\"\"Implements the Apriori algorithm.\n",
    "\n",
    "    The Apriori algorithm will iteratively generate new candidate \n",
    "    k-itemsets using the frequent (k-1)-itemsets found in the previous \n",
    "    iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate \n",
    "        candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    F : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] R. Agrawal, R. Srikant, \"Fast Algorithms for Mining Association \n",
    "           Rules\", 1994.\n",
    "\n",
    "    \"\"\"\n",
    "    C1 = create_candidates(dataset)\n",
    "    D = list(map(set, dataset))\n",
    "    F1, support_data = support_prune(D, C1, min_support, verbose=False) # prune candidate 1-itemsets\n",
    "    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n",
    "    k = 2 # the itemset cardinality\n",
    "    while (len(F[k - 2]) > 0):\n",
    "        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n",
    "        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n",
    "        support_data.update(supK) # update the support counts to reflect pruning\n",
    "        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n",
    "        k += 1\n",
    "\n",
    "    if verbose:\n",
    "        # Print a list of all the frequent itemsets.\n",
    "        for kset in F:\n",
    "            for item in kset:\n",
    "                print(\"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  sup = \" + str(round(support_data[item], 3)))\n",
    "\n",
    "    return F, support_data\n",
    "\n",
    "def create_candidates(dataset, verbose=False):\n",
    "    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n",
    "    immutable and hashable).\n",
    "    \"\"\"\n",
    "    c1 = [] # list of all items in the database of transactions\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if not [item] in c1:\n",
    "                c1.append([item])\n",
    "    c1.sort()\n",
    "\n",
    "    if verbose:\n",
    "        # Print a list of all the candidate items.\n",
    "        print(\"\" \\\n",
    "            + \"{\" \\\n",
    "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
    "            + \"}\")\n",
    "\n",
    "    # Map c1 to a frozenset because it will be the key of a dictionary.\n",
    "    return list(map(frozenset, c1))\n",
    "\n",
    "def support_prune(dataset, candidates, min_support, verbose=False):\n",
    "    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n",
    "\n",
    "    By the apriori principle, if an itemset is frequent, then all of its \n",
    "    subsets must also be frequent. As a result, we can perform support-based \n",
    "    pruning to systematically control the exponential growth of candidate \n",
    "    itemsets. Thus, itemsets that do not meet the minimum support level are \n",
    "    pruned from the input list of itemsets (dataset).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    candidates : frozenset\n",
    "        The list of candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "    \"\"\"\n",
    "    sscnt = {} # set for support counts\n",
    "    for tid in dataset:\n",
    "        for can in candidates:\n",
    "            if can.issubset(tid):\n",
    "                sscnt.setdefault(can, 0)\n",
    "                sscnt[can] += 1\n",
    "\n",
    "    num_items = float(len(dataset)) # total number of transactions in the dataset\n",
    "    retlist = [] # array for unpruned itemsets\n",
    "    support_data = {} # set for support data for corresponding itemsets\n",
    "    for key in sscnt:\n",
    "        # Calculate the support of itemset key.\n",
    "        support = sscnt[key] / num_items\n",
    "        if support >= min_support:\n",
    "            retlist.insert(0, key)\n",
    "        support_data[key] = support\n",
    "\n",
    "    # Print a list of the pruned itemsets.\n",
    "    if verbose:\n",
    "        for kset in retlist:\n",
    "            for item in kset:\n",
    "                print(\"{\" + str(item) + \"}\")\n",
    "        print(\"\")\n",
    "        for key in sscnt:\n",
    "            print(\"\" \\\n",
    "                + \"{\" \\\n",
    "                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n",
    "                + \"}\" \\\n",
    "                + \":  sup = \" + str(support_data[key]))\n",
    "\n",
    "    return retlist, support_data\n",
    "\n",
    "def apriori_gen(freq_sets, k):\n",
    "    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
    "\n",
    "    This operation generates new candidate k-itemsets based on the frequent \n",
    "    (k-1)-itemsets found in the previous iteration. The candidate generation \n",
    "    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n",
    "    items are identical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_sets : list\n",
    "        The list of frequent (k-1)-itemsets.\n",
    "\n",
    "    k : integer\n",
    "        The cardinality of the current itemsets being evaluated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of merged frequent itemsets.\n",
    "    \"\"\"\n",
    "    retList = [] # list of merged frequent itemsets\n",
    "    lenLk = len(freq_sets) # number of frequent itemsets\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1, lenLk):\n",
    "            a=list(freq_sets[i])\n",
    "            b=list(freq_sets[j])\n",
    "            a.sort()\n",
    "            b.sort()\n",
    "            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n",
    "            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n",
    "\n",
    "            if F1 == F2: # if the first k-2 items are identical\n",
    "                # Merge the frequent itemsets.\n",
    "                retList.append(freq_sets[i] | freq_sets[j])\n",
    "\n",
    "    return retList\n",
    "\n",
    "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
    "    \"\"\"Generates a set of candidate rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A list of frequent itemsets (of a particular length).\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "    m = len(H[0])\n",
    "    if m == 1:\n",
    "        Hmp1 = calc_confidence(freq_set, H, support_data, rules, min_confidence, verbose)\n",
    "    if (len(freq_set) > (m+1)):\n",
    "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
    "        Hmp1 = calc_confidence(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
    "        if len(Hmp1) > 1:\n",
    "            # If there are candidate rules above the minimum confidence \n",
    "            # threshold, recurse on the list of these candidate rules.\n",
    "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
    "\n",
    "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
    "    \"\"\"Evaluates the generated rules.\n",
    "\n",
    "    One measurement for quantifying the goodness of association rules is \n",
    "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
    "    the support for P and H divided by the support for P \n",
    "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
    "    (thus P|H means all the items in set P or in set H).\n",
    "\n",
    "    To calculate the confidence, we iterate through the frequent itemsets and \n",
    "    associated support data. For each frequent itemset, we divide the support \n",
    "    of the itemset by the support of the antecedent (left-hand-side of the \n",
    "    rule).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A list of frequent itemsets (of a particular length).\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pruned_H : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
    "    for conseq in H: # iterate over the frequent itemsets\n",
    "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
    "        if conf >= min_confidence:\n",
    "            rules.append((freq_set - conseq, conseq, conf))\n",
    "            pruned_H.append(conseq)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \" ---> \" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
    "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
    "\n",
    "    return pruned_H\n",
    "\n",
    "def generate_rules(F, support_data, min_confidence=0.5, verbose=True):\n",
    "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
    "\n",
    "    For each frequent itemset, we calculate the confidence of using a\n",
    "    particular item as the rule consequent (right-hand-side of the rule). By \n",
    "    testing and merging the remaining rules, we recursively create a list of \n",
    "    pruned rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    F : list\n",
    "        A list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The corresponding support data for the frequent itemsets (L).\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rules : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for i in range(1, len(F)):\n",
    "        for freq_set in F[i]:\n",
    "            H1 = [frozenset([itemset]) for itemset in freq_set]\n",
    "            if (i > 1):\n",
    "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
    "            else:\n",
    "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load our dataset of grocery transactions, use the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('grocery.csv')\n",
    "D = list(map(set, dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _dataset_ is now a ndarray containing each of the 9835 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9835,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tropical fruit', 'yogurt', 'coffee']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _D_ Contains that dataset in a set format (which excludes duplicated items and sorts them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'citrus fruit', 'margarine', 'ready soups', 'semi-finished bread'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete the assignment below by making use of the provided funtions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You may use the notebook file attached with lesson 3 as a reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> How does the Apriori works? </p>\n",
    "Candidate set is generated using the create_candidate function to create a candidate itemset. The support of each item is determined by the initial iteration over the dataset. The support_prune function does this task. Next, we filter out the data that has a minimum support (which is a deciding factor, here 0.05). The support data is then used for generating the association rules that pass the minimum confidence threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Instant food products, UHT-milk, abrasive cleaner, artif. sweetener, baby cosmetics, baby food, bags, baking powder, bathroom cleaner, beef, berries, beverages, bottled beer, bottled water, brandy, brown bread, butter, butter milk, cake bar, candles, candy, canned beer, canned fish, canned fruit, canned vegetables, cat food, cereals, chewing gum, chicken, chocolate, chocolate marshmallow, citrus fruit, cleaner, cling film/bags, cocoa drinks, coffee, condensed milk, cooking chocolate, cookware, cream, cream cheese , curd, curd cheese, decalcifier, dental care, dessert, detergent, dish cleaner, dishes, dog food, domestic eggs, female sanitary products, finished products, fish, flour, flower (seeds), flower soil/fertilizer, frankfurter, frozen chicken, frozen dessert, frozen fish, frozen fruits, frozen meals, frozen potato products, frozen vegetables, fruit/vegetable juice, grapes, hair spray, ham, hamburger meat, hard cheese, herbs, honey, house keeping products, hygiene articles, ice cream, instant coffee, jam, ketchup, kitchen towels, kitchen utensil, light bulbs, liqueur, liquor, liquor (appetizer), liver loaf, long life bakery product, make up remover, male cosmetics, margarine, mayonnaise, meat, meat spreads, misc. beverages, mustard, napkins, newspapers, nut snack, nuts/prunes, oil, onions, organic products, organic sausage, other vegetables, packaged fruit/vegetables, pasta, pastry, pet care, photo/film, pickled vegetables, pip fruit, popcorn, pork, pot plants, potato products, preservation products, processed cheese, prosecco, pudding powder, ready soups, red/blush wine, rice, roll products , rolls/buns, root vegetables, rubbing alcohol, rum, salad dressing, salt, salty snack, sauces, sausage, seasonal products, semi-finished bread, shopping bags, skin care, sliced cheese, snack products, soap, soda, soft cheese, softener, sound storage medium, soups, sparkling wine, specialty bar, specialty cheese, specialty chocolate, specialty fat, specialty vegetables, spices, spread cheese, sugar, sweet spreads, syrup, tea, tidbits, toilet cleaner, tropical fruit, turkey, vinegar, waffles, whipped/sour cream, whisky, white bread, white wine, whole milk, yogurt, zwieback}\n"
     ]
    }
   ],
   "source": [
    "# Generate candidate itemsets.\n",
    "# candidate 1-itemsets\n",
    "C1 = create_candidates(dataset, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{citrus fruit}:  sup = 0.08276563294356888\n",
      "{margarine}:  sup = 0.05856634468734113\n",
      "{ready soups}:  sup = 0.0018301982714794102\n",
      "{semi-finished bread}:  sup = 0.017691916624300967\n",
      "{coffee}:  sup = 0.05805795627859685\n",
      "{tropical fruit}:  sup = 0.10493136756481952\n",
      "{yogurt}:  sup = 0.13950177935943062\n",
      "{whole milk}:  sup = 0.25551601423487547\n",
      "{cream cheese}:  sup = 0.03965429588205389\n",
      "{meat spreads}:  sup = 0.004270462633451958\n",
      "{pip fruit}:  sup = 0.07564819522114896\n",
      "{condensed milk}:  sup = 0.010269445856634469\n",
      "{long life bakery product}:  sup = 0.037417386883579054\n",
      "{other vegetables}:  sup = 0.1934926283680732\n",
      "{abrasive cleaner}:  sup = 0.0035587188612099642\n",
      "{butter}:  sup = 0.05541433655312659\n",
      "{rice}:  sup = 0.007625826131164209\n",
      "{rolls/buns}:  sup = 0.18393492628368074\n",
      "{UHT-milk}:  sup = 0.03345195729537367\n",
      "{bottled beer}:  sup = 0.08052872394509406\n",
      "{liquor (appetizer)}:  sup = 0.007930859176410779\n",
      "{pot plants}:  sup = 0.01728520589730554\n",
      "{cereals}:  sup = 0.0056939501779359435\n",
      "{bottled water}:  sup = 0.11052364006100661\n",
      "{chocolate}:  sup = 0.04961870869344179\n",
      "{white bread}:  sup = 0.042094560244026434\n",
      "{curd}:  sup = 0.05327910523640061\n",
      "{dishes}:  sup = 0.01759023894255211\n",
      "{flour}:  sup = 0.017386883579054397\n",
      "{beef}:  sup = 0.05246568378240976\n",
      "{frankfurter}:  sup = 0.058973055414336555\n",
      "{soda}:  sup = 0.17437722419928825\n",
      "{chicken}:  sup = 0.04290798169801729\n",
      "{fruit/vegetable juice}:  sup = 0.0722928317234367\n",
      "{newspapers}:  sup = 0.07981698017285206\n",
      "{sugar}:  sup = 0.03385866802236909\n",
      "{packaged fruit/vegetables}:  sup = 0.013014743263853584\n",
      "{specialty bar}:  sup = 0.027351296390442297\n",
      "{butter milk}:  sup = 0.027961362480935434\n",
      "{pastry}:  sup = 0.08896797153024912\n",
      "{detergent}:  sup = 0.019217081850533807\n",
      "{processed cheese}:  sup = 0.016573462125063547\n",
      "{bathroom cleaner}:  sup = 0.0027452974072191155\n",
      "{candy}:  sup = 0.0298932384341637\n",
      "{frozen dessert}:  sup = 0.010777834265378749\n",
      "{root vegetables}:  sup = 0.10899847483477376\n",
      "{salty snack}:  sup = 0.03782409761057448\n",
      "{sweet spreads}:  sup = 0.009049313675648195\n",
      "{waffles}:  sup = 0.038434163701067614\n",
      "{canned beer}:  sup = 0.07768174885612608\n",
      "{sausage}:  sup = 0.09395017793594305\n",
      "{brown bread}:  sup = 0.06487036095577021\n",
      "{shopping bags}:  sup = 0.09852567361464158\n",
      "{beverages}:  sup = 0.026029486527707167\n",
      "{hamburger meat}:  sup = 0.033248601931875954\n",
      "{hygiene articles}:  sup = 0.03294356888662939\n",
      "{napkins}:  sup = 0.05236400610066091\n",
      "{spices}:  sup = 0.005185561769191663\n",
      "{artif. sweetener}:  sup = 0.003253685815963396\n",
      "{berries}:  sup = 0.033248601931875954\n",
      "{pork}:  sup = 0.05765124555160142\n",
      "{whipped/sour cream}:  sup = 0.07168276563294357\n",
      "{grapes}:  sup = 0.022369089984748347\n",
      "{dessert}:  sup = 0.03711235383833249\n",
      "{zwieback}:  sup = 0.006914082358922217\n",
      "{domestic eggs}:  sup = 0.06344687341128623\n",
      "{spread cheese}:  sup = 0.011184544992374174\n",
      "{misc. beverages}:  sup = 0.02836807320793086\n",
      "{hard cheese}:  sup = 0.024504321301474327\n",
      "{cat food}:  sup = 0.023284189120488054\n",
      "{ham}:  sup = 0.026029486527707167\n",
      "{baking powder}:  sup = 0.017691916624300967\n",
      "{turkey}:  sup = 0.00813421453990849\n",
      "{pickled vegetables}:  sup = 0.017895271987798677\n",
      "{chewing gum}:  sup = 0.021047280122013217\n",
      "{chocolate marshmallow}:  sup = 0.009049313675648195\n",
      "{oil}:  sup = 0.02806304016268429\n",
      "{ice cream}:  sup = 0.025012709710218607\n",
      "{canned fish}:  sup = 0.015048296898830707\n",
      "{frozen vegetables}:  sup = 0.04809354346720895\n",
      "{seasonal products}:  sup = 0.014234875444839857\n",
      "{curd cheese}:  sup = 0.005083884087442806\n",
      "{red/blush wine}:  sup = 0.019217081850533807\n",
      "{frozen potato products}:  sup = 0.008439247585155059\n",
      "{candles}:  sup = 0.008947635993899338\n",
      "{flower (seeds)}:  sup = 0.010371123538383325\n",
      "{specialty chocolate}:  sup = 0.03040162684290798\n",
      "{specialty fat}:  sup = 0.0036603965429588205\n",
      "{sparkling wine}:  sup = 0.005592272496187087\n",
      "{salt}:  sup = 0.010777834265378749\n",
      "{frozen meals}:  sup = 0.02836807320793086\n",
      "{canned vegetables}:  sup = 0.010777834265378749\n",
      "{onions}:  sup = 0.031011692933401117\n",
      "{herbs}:  sup = 0.01626842907981698\n",
      "{white wine}:  sup = 0.019013726487036097\n",
      "{brandy}:  sup = 0.004168784951703101\n",
      "{photo/film}:  sup = 0.009252669039145907\n",
      "{sliced cheese}:  sup = 0.024504321301474327\n",
      "{pasta}:  sup = 0.015048296898830707\n",
      "{softener}:  sup = 0.005490594814438231\n",
      "{cling film/bags}:  sup = 0.011387900355871887\n",
      "{fish}:  sup = 0.0029486527707168276\n",
      "{male cosmetics}:  sup = 0.004575495678698526\n",
      "{canned fruit}:  sup = 0.003253685815963396\n",
      "{Instant food products}:  sup = 0.008032536858159633\n",
      "{soft cheese}:  sup = 0.01708185053380783\n",
      "{honey}:  sup = 0.001525165226232842\n",
      "{dental care}:  sup = 0.005795627859684799\n",
      "{popcorn}:  sup = 0.007219115404168785\n",
      "{cake bar}:  sup = 0.013218098627351297\n",
      "{snack products}:  sup = 0.003050330452465684\n",
      "{flower soil/fertilizer}:  sup = 0.0019318759532282665\n",
      "{specialty cheese}:  sup = 0.008540925266903915\n",
      "{finished products}:  sup = 0.006507371631926792\n",
      "{cocoa drinks}:  sup = 0.0022369089984748346\n",
      "{dog food}:  sup = 0.008540925266903915\n",
      "{prosecco}:  sup = 0.0020335536349771225\n",
      "{frozen fish}:  sup = 0.011692933401118455\n",
      "{make up remover}:  sup = 0.000813421453990849\n",
      "{cleaner}:  sup = 0.005083884087442806\n",
      "{female sanitary products}:  sup = 0.006100660904931368\n",
      "{cookware}:  sup = 0.0027452974072191155\n",
      "{dish cleaner}:  sup = 0.01047280122013218\n",
      "{meat}:  sup = 0.025826131164209457\n",
      "{tea}:  sup = 0.003863751906456533\n",
      "{mustard}:  sup = 0.011997966446365024\n",
      "{house keeping products}:  sup = 0.008337569903406202\n",
      "{skin care}:  sup = 0.0035587188612099642\n",
      "{potato products}:  sup = 0.0028469750889679717\n",
      "{liquor}:  sup = 0.011082867310625319\n",
      "{pet care}:  sup = 0.00945602440264362\n",
      "{soups}:  sup = 0.00681240467717336\n",
      "{rum}:  sup = 0.004473817996949669\n",
      "{salad dressing}:  sup = 0.000813421453990849\n",
      "{sauces}:  sup = 0.005490594814438231\n",
      "{vinegar}:  sup = 0.006507371631926792\n",
      "{soap}:  sup = 0.0026436197254702592\n",
      "{hair spray}:  sup = 0.0011184544992374173\n",
      "{instant coffee}:  sup = 0.007422470767666497\n",
      "{roll products}:  sup = 0.010269445856634469\n",
      "{mayonnaise}:  sup = 0.009150991357397052\n",
      "{rubbing alcohol}:  sup = 0.0010167768174885613\n",
      "{syrup}:  sup = 0.003253685815963396\n",
      "{liver loaf}:  sup = 0.005083884087442806\n",
      "{baby cosmetics}:  sup = 0.0006100660904931368\n",
      "{organic products}:  sup = 0.001626842907981698\n",
      "{nut snack}:  sup = 0.00315200813421454\n",
      "{kitchen towels}:  sup = 0.005998983223182512\n",
      "{frozen chicken}:  sup = 0.0006100660904931368\n",
      "{light bulbs}:  sup = 0.004168784951703101\n",
      "{ketchup}:  sup = 0.004270462633451958\n",
      "{jam}:  sup = 0.005388917132689374\n",
      "{decalcifier}:  sup = 0.001525165226232842\n",
      "{nuts/prunes}:  sup = 0.003355363497712252\n",
      "{liqueur}:  sup = 0.0009150991357397051\n",
      "{organic sausage}:  sup = 0.0022369089984748346\n",
      "{cream}:  sup = 0.0013218098627351296\n",
      "{toilet cleaner}:  sup = 0.0007117437722419929\n",
      "{specialty vegetables}:  sup = 0.0017285205897305542\n",
      "{baby food}:  sup = 0.00010167768174885612\n",
      "{pudding powder}:  sup = 0.002338586680223691\n",
      "{tidbits}:  sup = 0.002338586680223691\n",
      "{whisky}:  sup = 0.000813421453990849\n",
      "{frozen fruits}:  sup = 0.0012201321809862736\n",
      "{bags}:  sup = 0.0004067107269954245\n",
      "{cooking chocolate}:  sup = 0.002541942043721403\n",
      "{sound storage medium}:  sup = 0.00010167768174885612\n",
      "{kitchen utensil}:  sup = 0.0004067107269954245\n",
      "{preservation products}:  sup = 0.00020335536349771224\n"
     ]
    }
   ],
   "source": [
    "# Prune candidate (support-based pruning) to generate frequent 1-itemsets.\n",
    "F1, support_data = support_prune(D, C1, 0.6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{domestic eggs}:  sup = 0.063\n",
      "{whipped/sour cream}:  sup = 0.072\n",
      "{pork}:  sup = 0.058\n",
      "{napkins}:  sup = 0.052\n",
      "{shopping bags}:  sup = 0.099\n",
      "{brown bread}:  sup = 0.065\n",
      "{sausage}:  sup = 0.094\n",
      "{canned beer}:  sup = 0.078\n",
      "{root vegetables}:  sup = 0.109\n",
      "{pastry}:  sup = 0.089\n",
      "{newspapers}:  sup = 0.08\n",
      "{fruit/vegetable juice}:  sup = 0.072\n",
      "{soda}:  sup = 0.174\n",
      "{frankfurter}:  sup = 0.059\n",
      "{beef}:  sup = 0.052\n",
      "{curd}:  sup = 0.053\n",
      "{bottled water}:  sup = 0.111\n",
      "{bottled beer}:  sup = 0.081\n",
      "{rolls/buns}:  sup = 0.184\n",
      "{butter}:  sup = 0.055\n",
      "{other vegetables}:  sup = 0.193\n",
      "{pip fruit}:  sup = 0.076\n",
      "{whole milk}:  sup = 0.256\n",
      "{yogurt}:  sup = 0.14\n",
      "{tropical fruit}:  sup = 0.105\n",
      "{coffee}:  sup = 0.058\n",
      "{margarine}:  sup = 0.059\n",
      "{citrus fruit}:  sup = 0.083\n",
      "{whole milk, rolls/buns}:  sup = 0.057\n",
      "{yogurt, whole milk}:  sup = 0.056\n",
      "{whole milk, other vegetables}:  sup = 0.075\n"
     ]
    }
   ],
   "source": [
    "# Generate all the frequent itemsets using the Apriori algorithm.\n",
    "F, support_data = apriori(dataset, min_support=0.05, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Given a frequent itemset (here, extracted by the Apriori algorithm), we can generate the association rules with high support and confidence (via the generate_rules function)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{rolls/buns} ---> {whole milk}:  conf = 0.308, sup = 0.057\n",
      "{whole milk} ---> {rolls/buns}:  conf = 0.222, sup = 0.057\n",
      "{whole milk} ---> {yogurt}:  conf = 0.219, sup = 0.056\n",
      "{yogurt} ---> {whole milk}:  conf = 0.402, sup = 0.056\n",
      "{other vegetables} ---> {whole milk}:  conf = 0.387, sup = 0.075\n",
      "{whole milk} ---> {other vegetables}:  conf = 0.293, sup = 0.075\n"
     ]
    }
   ],
   "source": [
    "# Generate the association rules from a list of frequent itemsets.\n",
    "H = generate_rules(F, support_data, min_confidence=0.05, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a very interesting set of associations in the result above. When we see from the perpective of a customer buying groceries from a grocery store, these are indeed closely associated items that are frequently bought together. Here I took support = 0.05 and confidence = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{onions}:  sup = 0.031\n",
      "{specialty chocolate}:  sup = 0.03\n",
      "{frozen vegetables}:  sup = 0.048\n",
      "{domestic eggs}:  sup = 0.063\n",
      "{dessert}:  sup = 0.037\n",
      "{whipped/sour cream}:  sup = 0.072\n",
      "{pork}:  sup = 0.058\n",
      "{berries}:  sup = 0.033\n",
      "{napkins}:  sup = 0.052\n",
      "{hygiene articles}:  sup = 0.033\n",
      "{hamburger meat}:  sup = 0.033\n",
      "{shopping bags}:  sup = 0.099\n",
      "{brown bread}:  sup = 0.065\n",
      "{sausage}:  sup = 0.094\n",
      "{canned beer}:  sup = 0.078\n",
      "{waffles}:  sup = 0.038\n",
      "{salty snack}:  sup = 0.038\n",
      "{root vegetables}:  sup = 0.109\n",
      "{pastry}:  sup = 0.089\n",
      "{sugar}:  sup = 0.034\n",
      "{newspapers}:  sup = 0.08\n",
      "{fruit/vegetable juice}:  sup = 0.072\n",
      "{chicken}:  sup = 0.043\n",
      "{soda}:  sup = 0.174\n",
      "{frankfurter}:  sup = 0.059\n",
      "{beef}:  sup = 0.052\n",
      "{curd}:  sup = 0.053\n",
      "{white bread}:  sup = 0.042\n",
      "{chocolate}:  sup = 0.05\n",
      "{bottled water}:  sup = 0.111\n",
      "{bottled beer}:  sup = 0.081\n",
      "{UHT-milk}:  sup = 0.033\n",
      "{rolls/buns}:  sup = 0.184\n",
      "{butter}:  sup = 0.055\n",
      "{other vegetables}:  sup = 0.193\n",
      "{long life bakery product}:  sup = 0.037\n",
      "{pip fruit}:  sup = 0.076\n",
      "{cream cheese}:  sup = 0.04\n",
      "{whole milk}:  sup = 0.256\n",
      "{yogurt}:  sup = 0.14\n",
      "{tropical fruit}:  sup = 0.105\n",
      "{coffee}:  sup = 0.058\n",
      "{margarine}:  sup = 0.059\n",
      "{citrus fruit}:  sup = 0.083\n",
      "{yogurt, other vegetables}:  sup = 0.043\n",
      "{pip fruit, whole milk}:  sup = 0.03\n",
      "{whole milk, rolls/buns}:  sup = 0.057\n",
      "{yogurt, rolls/buns}:  sup = 0.034\n",
      "{whole milk, pastry}:  sup = 0.033\n",
      "{soda, whole milk}:  sup = 0.04\n",
      "{soda, other vegetables}:  sup = 0.033\n",
      "{whole milk, whipped/sour cream}:  sup = 0.032\n",
      "{root vegetables, whole milk}:  sup = 0.049\n",
      "{sausage, rolls/buns}:  sup = 0.031\n",
      "{root vegetables, other vegetables}:  sup = 0.047\n",
      "{soda, rolls/buns}:  sup = 0.038\n",
      "{whole milk, citrus fruit}:  sup = 0.031\n",
      "{whole milk, tropical fruit}:  sup = 0.042\n",
      "{bottled water, whole milk}:  sup = 0.034\n",
      "{tropical fruit, other vegetables}:  sup = 0.036\n",
      "{rolls/buns, other vegetables}:  sup = 0.043\n",
      "{yogurt, whole milk}:  sup = 0.056\n",
      "{whole milk, other vegetables}:  sup = 0.075\n"
     ]
    }
   ],
   "source": [
    "F, support_data = apriori(dataset, min_support=0.03, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{other vegetables} ---> {yogurt}:  conf = 0.224, sup = 0.043\n",
      "{yogurt} ---> {other vegetables}:  conf = 0.311, sup = 0.043\n",
      "{whole milk} ---> {pip fruit}:  conf = 0.118, sup = 0.03\n",
      "{pip fruit} ---> {whole milk}:  conf = 0.398, sup = 0.03\n",
      "{rolls/buns} ---> {whole milk}:  conf = 0.308, sup = 0.057\n",
      "{whole milk} ---> {rolls/buns}:  conf = 0.222, sup = 0.057\n",
      "{rolls/buns} ---> {yogurt}:  conf = 0.187, sup = 0.034\n",
      "{yogurt} ---> {rolls/buns}:  conf = 0.246, sup = 0.034\n",
      "{pastry} ---> {whole milk}:  conf = 0.374, sup = 0.033\n",
      "{whole milk} ---> {pastry}:  conf = 0.13, sup = 0.033\n",
      "{whole milk} ---> {soda}:  conf = 0.157, sup = 0.04\n",
      "{soda} ---> {whole milk}:  conf = 0.23, sup = 0.04\n",
      "{other vegetables} ---> {soda}:  conf = 0.169, sup = 0.033\n",
      "{soda} ---> {other vegetables}:  conf = 0.188, sup = 0.033\n",
      "{whipped/sour cream} ---> {whole milk}:  conf = 0.45, sup = 0.032\n",
      "{whole milk} ---> {whipped/sour cream}:  conf = 0.126, sup = 0.032\n",
      "{whole milk} ---> {root vegetables}:  conf = 0.191, sup = 0.049\n",
      "{root vegetables} ---> {whole milk}:  conf = 0.449, sup = 0.049\n",
      "{rolls/buns} ---> {sausage}:  conf = 0.166, sup = 0.031\n",
      "{sausage} ---> {rolls/buns}:  conf = 0.326, sup = 0.031\n",
      "{other vegetables} ---> {root vegetables}:  conf = 0.245, sup = 0.047\n",
      "{root vegetables} ---> {other vegetables}:  conf = 0.435, sup = 0.047\n",
      "{rolls/buns} ---> {soda}:  conf = 0.208, sup = 0.038\n",
      "{soda} ---> {rolls/buns}:  conf = 0.22, sup = 0.038\n",
      "{citrus fruit} ---> {whole milk}:  conf = 0.369, sup = 0.031\n",
      "{whole milk} ---> {citrus fruit}:  conf = 0.119, sup = 0.031\n",
      "{tropical fruit} ---> {whole milk}:  conf = 0.403, sup = 0.042\n",
      "{whole milk} ---> {tropical fruit}:  conf = 0.166, sup = 0.042\n",
      "{whole milk} ---> {bottled water}:  conf = 0.135, sup = 0.034\n",
      "{bottled water} ---> {whole milk}:  conf = 0.311, sup = 0.034\n",
      "{other vegetables} ---> {tropical fruit}:  conf = 0.185, sup = 0.036\n",
      "{tropical fruit} ---> {other vegetables}:  conf = 0.342, sup = 0.036\n",
      "{other vegetables} ---> {rolls/buns}:  conf = 0.22, sup = 0.043\n",
      "{rolls/buns} ---> {other vegetables}:  conf = 0.232, sup = 0.043\n",
      "{whole milk} ---> {yogurt}:  conf = 0.219, sup = 0.056\n",
      "{yogurt} ---> {whole milk}:  conf = 0.402, sup = 0.056\n",
      "{other vegetables} ---> {whole milk}:  conf = 0.387, sup = 0.075\n",
      "{whole milk} ---> {other vegetables}:  conf = 0.293, sup = 0.075\n"
     ]
    }
   ],
   "source": [
    "H = generate_rules(F, support_data, min_confidence=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I decreased the value of support, and increased the value of confidence, we see further interesting results. The set of closely related items or associated items increases further. This is because we decreased the support threshold of the dataset.\n",
    "\n",
    "When the dataset is very huge, thousands of transactions, it is difficult to find the itemset with high minimum support. Thus to see interesting results, I put the minimum support value 0.05 and 0.03. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://localhost:8888/notebooks/Suhani/Fall%202018/DMT/cse40647/10%20-%20Apriori.ipynb\n",
    "2. https://www.northeastern.edu/mscs_online/cs6220-18501-fall-2018/\n",
    "3. http://localhost:8889/notebooks/Suhani/Fall%202018/DMT/cs6220/M04-A01%20-%20Apriori%20Helper%20Functions.ipynb\n",
    "4. http://localhost:8889/notebooks/Suhani/Fall%202018/DMT/cs6220/M04-N01%20-%20Apriori.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
